{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "20e0a443",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/facebookresearch/CrypTen.git\n",
      "  Cloning https://github.com/facebookresearch/CrypTen.git to c:\\users\\work\\appdata\\local\\temp\\pip-req-build-1vgdfp7i\n",
      "  Resolved https://github.com/facebookresearch/CrypTen.git to commit 775868a02d6dac50774ce376a55b01fbd8bd85b6\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Requirement already satisfied: torch>=1.7.0 in c:\\users\\work\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from crypten==0.4.0) (2.8.0)\n",
      "Collecting torchvision>=0.9.1 (from crypten==0.4.0)\n",
      "  Downloading torchvision-0.23.0-cp313-cp313-win_amd64.whl.metadata (6.1 kB)\n",
      "Collecting omegaconf>=2.0.6 (from crypten==0.4.0)\n",
      "  Downloading omegaconf-2.3.0-py3-none-any.whl.metadata (3.9 kB)\n",
      "Collecting onnx>=1.7.0 (from crypten==0.4.0)\n",
      "  Downloading onnx-1.19.0-cp313-cp313-win_amd64.whl.metadata (7.2 kB)\n",
      "Requirement already satisfied: pandas>=1.2.2 in c:\\users\\work\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from crypten==0.4.0) (2.3.1)\n",
      "Collecting pyyaml>=5.3.1 (from crypten==0.4.0)\n",
      "  Downloading PyYAML-6.0.2-cp313-cp313-win_amd64.whl.metadata (2.1 kB)\n",
      "Collecting tensorboard (from crypten==0.4.0)\n",
      "  Downloading tensorboard-2.20.0-py3-none-any.whl.metadata (1.8 kB)\n",
      "Collecting future (from crypten==0.4.0)\n",
      "  Downloading future-1.0.0-py3-none-any.whl.metadata (4.0 kB)\n",
      "Requirement already satisfied: scipy>=1.6.0 in c:\\users\\work\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from crypten==0.4.0) (1.16.1)\n",
      "Collecting sklearn (from crypten==0.4.0)\n",
      "  Downloading sklearn-0.0.post12.tar.gz (2.6 kB)\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'error'\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  Running command git clone --filter=blob:none --quiet https://github.com/facebookresearch/CrypTen.git 'C:\\Users\\Work\\AppData\\Local\\Temp\\pip-req-build-1vgdfp7i'\n",
      "  error: subprocess-exited-with-error\n",
      "  \n",
      "  × Getting requirements to build wheel did not run successfully.\n",
      "  │ exit code: 1\n",
      "  ╰─> [15 lines of output]\n",
      "      The 'sklearn' PyPI package is deprecated, use 'scikit-learn'\n",
      "      rather than 'sklearn' for pip commands.\n",
      "      \n",
      "      Here is how to fix this error in the main use cases:\n",
      "      - use 'pip install scikit-learn' rather than 'pip install sklearn'\n",
      "      - replace 'sklearn' by 'scikit-learn' in your pip requirements files\n",
      "        (requirements.txt, setup.py, setup.cfg, Pipfile, etc ...)\n",
      "      - if the 'sklearn' package is used by one of your dependencies,\n",
      "        it would be great if you take some time to track which package uses\n",
      "        'sklearn' instead of 'scikit-learn' and report it to their issue tracker\n",
      "      - as a last resort, set the environment variable\n",
      "        SKLEARN_ALLOW_DEPRECATED_SKLEARN_PACKAGE_INSTALL=True to avoid this error\n",
      "      \n",
      "      More information is available at\n",
      "      https://github.com/scikit-learn/sklearn-pypi-package\n",
      "      [end of output]\n",
      "  \n",
      "  note: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "error: subprocess-exited-with-error\n",
      "\n",
      "× Getting requirements to build wheel did not run successfully.\n",
      "│ exit code: 1\n",
      "╰─> See above for output.\n",
      "\n",
      "note: This error originates from a subprocess, and is likely not a problem with pip.\n"
     ]
    }
   ],
   "source": [
    "%pip install git+https://github.com/facebookresearch/CrypTen.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c31a5778",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tenseal\n",
      "  Downloading tenseal-0.3.16-cp313-cp313-win_amd64.whl.metadata (8.6 kB)\n",
      "Downloading tenseal-0.3.16-cp313-cp313-win_amd64.whl (2.2 MB)\n",
      "   ---------------------------------------- 0.0/2.2 MB ? eta -:--:--\n",
      "   -------------- ------------------------- 0.8/2.2 MB 7.4 MB/s eta 0:00:01\n",
      "   ------------------- -------------------- 1.0/2.2 MB 3.9 MB/s eta 0:00:01\n",
      "   ------------------- -------------------- 1.0/2.2 MB 3.9 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 1.8/2.2 MB 2.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.2/2.2 MB 2.2 MB/s  0:00:00\n",
      "Installing collected packages: tenseal\n",
      "Successfully installed tenseal-0.3.16\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install tenseal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "85ca1d5f",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'crypten'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mcrypten\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtenseal\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mts\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mCrypTen version:\u001b[39m\u001b[33m\"\u001b[39m, crypten.__version__)\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'crypten'"
     ]
    }
   ],
   "source": [
    "import crypten\n",
    "import tenseal as ts\n",
    "\n",
    "print(\"CrypTen version:\", crypten.__version__)\n",
    "print(\"TenSEAL loaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fa4d90b",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "scale out of bounds",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 110\u001b[39m\n\u001b[32m    104\u001b[39m     t_plain, m_plain = plaintext_inference(model, x)\n\u001b[32m    106\u001b[39m     \u001b[38;5;66;03m# # CrypTen\u001b[39;00m\n\u001b[32m    107\u001b[39m     \u001b[38;5;66;03m# t_crypten, m_crypten = crypten_inference(model, x)\u001b[39;00m\n\u001b[32m    108\u001b[39m \n\u001b[32m    109\u001b[39m     \u001b[38;5;66;03m# TenSEAL\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m110\u001b[39m     t_tenseal, m_tenseal = \u001b[43mtenseal_inference\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    112\u001b[39m     results.append({\n\u001b[32m    113\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33minput_dim\u001b[39m\u001b[33m\"\u001b[39m: dim,\n\u001b[32m    114\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mplaintext_time_ms\u001b[39m\u001b[33m\"\u001b[39m: t_plain,\n\u001b[32m   (...)\u001b[39m\u001b[32m    119\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mtenseal_mem_mb\u001b[39m\u001b[33m\"\u001b[39m: m_tenseal\n\u001b[32m    120\u001b[39m     })\n\u001b[32m    122\u001b[39m results_df = pd.DataFrame(results)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 88\u001b[39m, in \u001b[36mtenseal_inference\u001b[39m\u001b[34m(model, x)\u001b[39m\n\u001b[32m     86\u001b[39m start_mem = measure_memory()\n\u001b[32m     87\u001b[39m start = time.time()\n\u001b[32m---> \u001b[39m\u001b[32m88\u001b[39m enc_out = \u001b[43mencrypted_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43menc_x\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     89\u001b[39m end = time.time()\n\u001b[32m     90\u001b[39m end_mem = measure_memory()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 81\u001b[39m, in \u001b[36mtenseal_inference.<locals>.encrypted_forward\u001b[39m\u001b[34m(enc_vec, model)\u001b[39m\n\u001b[32m     79\u001b[39m     enc_vec = enc_vec.mm(W.T) + b.tolist()\n\u001b[32m     80\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(layer, nn.ReLU):\n\u001b[32m---> \u001b[39m\u001b[32m81\u001b[39m     enc_vec = \u001b[43mrelu_poly\u001b[49m\u001b[43m(\u001b[49m\u001b[43menc_vec\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     82\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(layer, nn.Sigmoid):\n\u001b[32m     83\u001b[39m     enc_vec = sigmoid_poly(enc_vec)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 70\u001b[39m, in \u001b[36mtenseal_inference.<locals>.relu_poly\u001b[39m\u001b[34m(enc_vec)\u001b[39m\n\u001b[32m     69\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mrelu_poly\u001b[39m(enc_vec):\n\u001b[32m---> \u001b[39m\u001b[32m70\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43menc_vec\u001b[49m\u001b[43m.\u001b[49m\u001b[43msquare\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Work\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\tenseal\\tensors\\abstract_tensor.py:160\u001b[39m, in \u001b[36mAbstractTensor.square\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    159\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34msquare\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> \u001b[33m\"\u001b[39m\u001b[33mAbstractTensor\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m160\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._wrap(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m.\u001b[49m\u001b[43msquare\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[31mValueError\u001b[39m: scale out of bounds"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import tenseal as ts\n",
    "import time, os, psutil\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# --- Define PyTorch Model (same architecture) ---\n",
    "class DeepNN(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(DeepNN, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(input_dim, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 48),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(48, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 16),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(16, 8),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(8, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "# --- Helpers ---\n",
    "def measure_memory():\n",
    "    process = psutil.Process(os.getpid())\n",
    "    return process.memory_info().rss / (1024 * 1024)  # MB\n",
    "\n",
    "def plaintext_inference(model, x):\n",
    "    start_mem = measure_memory()\n",
    "    start = time.time()\n",
    "    _ = model(x)\n",
    "    end = time.time()\n",
    "    end_mem = measure_memory()\n",
    "    return (end - start) * 1000, (end_mem - start_mem)  # ms, MB\n",
    "\n",
    "# ---------- TenSEAL Inference (with depth/scale control) ----------\n",
    "def tenseal_inference(model, x):\n",
    "    \"\"\"\n",
    "    Encrypted inference using CKKS with careful scale management:\n",
    "      - Large initial scale (2**40)\n",
    "      - Long modulus chain for ~10 rescale levels\n",
    "      - ReLU -> (x * 0.25)^2  (keeps magnitude small)\n",
    "      - Sigmoid -> 0.5 + 0.125 * x  (degree-1 proxy to preserve depth)\n",
    "    \"\"\"\n",
    "    # 1) Context with deeper modulus chain to support many rescale steps\n",
    "    context = ts.context(\n",
    "        ts.SCHEME_TYPE.CKKS,\n",
    "        poly_modulus_degree=16384,  # more slots & noise budget\n",
    "        coeff_mod_bit_sizes=[60, 40, 40, 40, 40, 40, 40, 40, 40, 60]  # ~8 rescale levels\n",
    "    )\n",
    "    context.global_scale = 2**40\n",
    "    context.generate_galois_keys()\n",
    "\n",
    "    # 2) Encrypt input (optionally shrink magnitude to help stability)\n",
    "    x_plain = x.tolist()[0]\n",
    "    enc_vec = ts.ckks_vector(context, x_plain)\n",
    "\n",
    "    # ----- Activation proxies -----\n",
    "    def relu_proxy(v):\n",
    "        # tamed square: (x * 0.25)^2\n",
    "        v = v.mul_plain(0.25)        # multiply by plaintext scalar (adds a level, small)\n",
    "        v = v.square()               # ciphertext-ciphertext mult (adds a level)\n",
    "        v.rescale_next()\n",
    "        return v\n",
    "\n",
    "    def sigmoid_proxy(v):\n",
    "        # degree-1 proxy: 0.5 + 0.125 * x\n",
    "        v = v.mul_plain(0.125)       # multiply by small scalar\n",
    "        v.rescale_next()\n",
    "        v = v.add_plain(0.5)\n",
    "        return v\n",
    "\n",
    "    # 3) Encrypted forward with explicit rescale after each mult\n",
    "    def encrypted_forward(enc_v, model):\n",
    "        for layer in model.model:\n",
    "            if isinstance(layer, nn.Linear):\n",
    "                W = layer.weight.detach().numpy()  # (out, in)\n",
    "                b = layer.bias.detach().numpy()    # (out,)\n",
    "                # enc_v.mm does ciphertext * plaintext-matrix (adds one mult level)\n",
    "                enc_v = enc_v.mm(W.T)              # -> (out,)\n",
    "                enc_v.rescale_next()\n",
    "                enc_v = enc_v.add_plain(b.tolist())  # add doesn't consume a level\n",
    "            elif isinstance(layer, nn.ReLU):\n",
    "                enc_v = relu_proxy(enc_v)\n",
    "            elif isinstance(layer, nn.Sigmoid):\n",
    "                enc_v = sigmoid_proxy(enc_v)\n",
    "            else:\n",
    "                # No-ops for layers that aren't used here\n",
    "                pass\n",
    "        return enc_v\n",
    "\n",
    "    start_mem = measure_memory()\n",
    "    start = time.time()\n",
    "    enc_out = encrypted_forward(enc_vec, model)\n",
    "    end = time.time()\n",
    "    end_mem = measure_memory()\n",
    "\n",
    "    # Optional: decrypt to check (won't affect timings above)\n",
    "    _ = enc_out.decrypt()\n",
    "\n",
    "    return (end - start) * 1000, (end_mem - start_mem)  # ms, MB\n",
    "\n",
    "# --- Run Experiments (TenSEAL vs Plaintext) ---\n",
    "input_dims = [15, 50, 100]\n",
    "results = []\n",
    "\n",
    "for dim in input_dims:\n",
    "    model = DeepNN(dim).eval()\n",
    "    x = torch.randn(1, dim)\n",
    "\n",
    "    # Plaintext baseline\n",
    "    t_plain, m_plain = plaintext_inference(model, x)\n",
    "\n",
    "    # TenSEAL encrypted\n",
    "    t_tenseal, m_tenseal = tenseal_inference(model, x)\n",
    "\n",
    "    results.append({\n",
    "        \"input_dim\": dim,\n",
    "        \"plaintext_time_ms\": t_plain,\n",
    "        \"tenseal_time_ms\": t_tenseal,\n",
    "        \"plaintext_mem_mb\": m_plain,\n",
    "        \"tenseal_mem_mb\": m_tenseal\n",
    "    })\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "print(results_df)\n",
    "\n",
    "# --- Plotting (TenSEAL vs Plaintext) ---\n",
    "def plot_performance_graphs(results_df_plot):\n",
    "    plt.style.use('seaborn-v0_8-darkgrid')\n",
    "    fig_width, fig_height = 10, 6\n",
    "    os.makedirs('figures', exist_ok=True)\n",
    "\n",
    "    # Time vs Input Size\n",
    "    plt.figure(figsize=(fig_width, fig_height))\n",
    "    plt.plot(results_df_plot['input_dim'], results_df_plot['plaintext_time_ms'], label='Plaintext NN', marker='o', color='blue')\n",
    "    plt.plot(results_df_plot['input_dim'], results_df_plot['tenseal_time_ms'], label='TenSEAL Encrypted NN', marker='s', color='green')\n",
    "    plt.xlabel('Input Dimension ($n$)')\n",
    "    plt.ylabel('Inference Time (ms)')\n",
    "    plt.title('Computation Time vs Input Dimension (Plain vs TenSEAL)')\n",
    "    plt.legend()\n",
    "    plt.xscale('log')\n",
    "    plt.yscale('log')\n",
    "    plt.grid(True, which=\"both\", ls=\"--\", c='0.7')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('./figures/time_vs_input_size.png')\n",
    "    plt.show()\n",
    "\n",
    "    # Memory vs Input Size\n",
    "    plt.figure(figsize=(fig_width, fig_height))\n",
    "    plt.plot(results_df_plot['input_dim'], results_df_plot['plaintext_mem_mb'], label='Plaintext NN (Activations)', marker='o', color='blue')\n",
    "    plt.plot(results_df_plot['input_dim'], results_df_plot['tenseal_mem_mb'], label='TenSEAL NN (Activations)', marker='s', color='green')\n",
    "    plt.xlabel('Input Dimension ($n$)')\n",
    "    plt.ylabel('Memory Usage (MB)')\n",
    "    plt.title('Activation Memory Usage vs Input Dimension (Plain vs TenSEAL)')\n",
    "    plt.legend()\n",
    "    plt.xscale('log')\n",
    "    plt.yscale('log')\n",
    "    plt.grid(True, which=\"both\", ls=\"--\", c='0.7')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('./figures/memory_vs_input_size.png')\n",
    "    plt.show()\n",
    "\n",
    "    # Relative Overhead vs Input Size\n",
    "    plt.figure(figsize=(fig_width, fig_height))\n",
    "    overhead_tenseal = ((results_df_plot['tenseal_time_ms'] - results_df_plot['plaintext_time_ms']) / results_df_plot['plaintext_time_ms']) * 100\n",
    "    plt.plot(results_df_plot['input_dim'], overhead_tenseal, label='TenSEAL Overhead (%)', marker='s', color='orange')\n",
    "    plt.xlabel('Input Dimension ($n$)')\n",
    "    plt.ylabel('Relative Overhead (%)')\n",
    "    plt.title('TenSEAL Relative Time Overhead vs Input Dimension')\n",
    "    plt.legend()\n",
    "    plt.xscale('log')\n",
    "    plt.grid(True, which=\"both\", ls=\"--\", c='0.7')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('./figures/relative_overhead_vs_input_size.png')\n",
    "    plt.show()\n",
    "\n",
    "plot_performance_graphs(results_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a589df0e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
